{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OK. We've spent some time thinking about how to get and work with data, but we haven't really touched on what you can do with data once you have it. The reason for this is that data munging and data analysis are really two separate concepts in their own way. And the kinds of analysis you can perform on data are as vast as the types of data you could find. As a digital humanist, you might be interested in any number of things: georeferencing, statistical measurements, network analysis, or many more. And, then, once you've analyzed things, you'll likely want to visualize your results. For the purposes of showing you what you can do with Python, we will just barely scratch the surface of these areas by showing some very basic methods. We will then visualize our results, which will hopefully show how you can use programming to carry out interpretations. Our goal here will be to use some of the data from the previous lesson on web scraping. Since the previous data was text, we will be working with basic text analysis to analyze author style by word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get the data we need by copying over some of the work that we did last time. First we will import the modules that we need. Then we will use Beautiful Soup to scrape down the corpus. Be sure to check out the previous lesson or ask a neighbor if you have any questions about what any of these lines are doing. This will take a few moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from contextlib import closing\n",
    "from urllib import request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/humanitiesprogramming/scraping-corpus/master/full-text.txt\"\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "raw_text = soup.text\n",
    "texts = eval(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eval() function is new here, and it tells Python to take a string it is passed and interpret it as code. Beautiful Soup pulls down the contents of a website, but it assumes that the result of soup.text is going to be a string. If you actually look at the contents of that link, though, you'll see that I dumped the contents of the texts as a list of texts. So we need to interpret that big long text file as code, and Python can help us do that. Calling eval on it looks for special characters like the [], which indicate lists, and runs Python on it as expected. To actually work with this code again. We can prove that this is going on by taking the length of our two different versions of the soup results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4398113\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_text))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first len() function is way larger, as it is taking the length of a giant string. So it returns the total number of characters in the collected text. The second statement gives us the expected result \"10\", because it is measuring the length of our list of texts. We have 10 of them. As always, it is important that we remember what data types we have and when we have them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we can start processing it as text. The package we are using is NLTK, the Natural Language Toolkit, which is something of a Swiss army knife for text analysis. Other packages might give you better baked in functionality, but NLTK is great for learning because it expects that you'll be working your own text functions from scratch. It also has a fantastic [text book](https://nltk.org/book) that I often use for teaching text analysis. The exercises rapidly engage you in real-world text questions. Let's start by importing what we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is a massive package, with lots of moving pieces. We'll be calling lots of lower-level functions, so expect to do a lot of dot typing. The second line here is a way of shortening that process so that instead of typing nltk.word_tokenize a lot we can just type word_tokenize. Before we work with NLTK, we'll actually need to download some more things to your computer. The following line will call a pop-up downloader interface so that we can download a bunch of texts baked into NLTK. You probably only need to run it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've pulled texts onto your computer, NLTK will give us access to them if we bring them into Python. Here we pull in the first text and the first ten words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "print(text1)\n",
    "text1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How fun! But wait, something is going on here - this text looks different than the raw text that we have. As we've been learning all along, computers have to work with structured data. By default, humanities texts are pretty darn unstructured. We could think of any number of ways to structure them, but the way done here is to break that text down into smaller units:\n",
    "\n",
    "* A text is made of many sentences. (Breaking down texts into sentences is called **segmentation**)\n",
    "* A Sentence is made of many words. (Breaking a large text into words is called **tokenization**, and those words become called **tokens**.\n",
    "\n",
    "Of course, this process quickly becomes subject to interpretation: are you going to count punctuation as tokens? The pre-packaged NLTK texts come with a lot of those decisions already made. We're going to go through the whole process ourselves so that you have a sense of how each part of it works. Here are the steps for the one we'll be using:\n",
    "\n",
    "* tokenization\n",
    "* normalization\n",
    "* removing stopwords\n",
    "* analysis\n",
    "* visualization\n",
    "\n",
    "Those are some basics, but, depending on your interests, you might have more steps. You might, for example care about sentence boundaries. Or, you might be interested in tagging the part of speech for each word. The process will change depending on your interests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The first step in our process is to break the text into smaller units that we can work with. In any tokenization process, you have to decide what kinds of things count as tokens - does punctuation count? How do we deal with word boundaries? You could tokenize things yourself, but it's not necessary to reinvent the wheel. We'll use NLTK to tokenize for us. \n",
    "\n",
    "This will take a bit of time to process, as we're working with a lot of text. So that you know things aren't broken, I've included a timer that prints out as it moves through each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\n",
      "128254\n",
      "['Project', 'Gutenberg', \"'s\", 'The', 'Return', 'of', 'Sherlock', 'Holmes', ',', 'by', 'Arthur', 'Conan', 'DoyleThis', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone']\n",
      "=====\n",
      "119498\n",
      "['Project', 'Gutenberg', \"'s\", 'The', 'Adventures', 'of', 'Sherlock', 'Holmes', ',', 'by', 'Arthur', 'Conan', 'DoyleThis', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone']\n",
      "=====\n",
      "52136\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'A', 'Study', 'In', 'Scarlet', ',', 'by', 'Arthur', 'Conan', 'DoyleThis', 'eBook', 'is', 'for', 'the', 'use', 'of']\n",
      "=====\n",
      "67933\n",
      "['Project', 'Gutenberg', \"'s\", 'The', 'Hound', 'of', 'the', 'Baskervilles', ',', 'by', 'A.', 'Conan', 'DoyleThis', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone']\n",
      "=====\n",
      "54520\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'The', 'Sign', 'of', 'the', 'Four', ',', 'by', 'Arthur', 'Conan', 'DoyleThis', 'eBook', 'is', 'for', 'the', 'use']\n",
      "=====\n",
      "214696\n",
      "['Jane', 'Eyre', ',', 'by', 'Charlotte', 'BronteThe', 'Project', 'Gutenberg', 'eBook', ',', 'Jane', 'Eyre', ',', 'by', 'Charlotte', 'Bronte', ',', 'Illustratedby', 'F.', 'H.']\n",
      "=====\n",
      "113812\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Villette', ',', 'by', 'Charlotte', 'BrontëThis', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no']\n",
      "=====\n",
      "12814\n",
      "['Project', 'Gutenberg', \"'s\", 'The', 'Search', 'After', 'Happiness', ',', 'by', 'Charlotte', 'BronteThis', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in']\n",
      "=====\n",
      "136931\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Shirley', ',', 'by', 'Charlotte', 'BrontëThis', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no']\n",
      "=====\n",
      "12814\n",
      "['Project', 'Gutenberg', \"'s\", 'The', 'Search', 'After', 'Happiness', ',', 'by', 'Charlotte', 'BronteThis', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokenized_texts = []\n",
    "for text in texts:\n",
    "    tokenized_texts.append(word_tokenize(text))\n",
    "\n",
    "for tokenized_text in tokenized_texts:\n",
    "    print('=====')\n",
    "    print(len(tokenized_text))\n",
    "    print(tokenized_text[0:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! We've got a series of texts, all of which are tokenized. But wow those are big numbers. Lots of words! Five texts by Charlotte Bronte and five by Sir Arthur Conan Doyle. Let's get a little more organized by separating the two corpora by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doyle = tokenized_texts[0:4]\n",
    "bronte = tokenized_texts[5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Humanities data is messy. And as we've often noted, computers don't deal with mess well. We'll take a few steps to help our friendly neighborhood computer. We'll do two things here:\n",
    "\n",
    "* lowercase all words (for a computer, \"The\" is a different word from \"the\")\n",
    "* remove the Project Gutenberg frontmatter (you may have noticed that all the texts above started the same way)\n",
    "\n",
    "For the second one, Project Gutenberg actually makes things a little tricky. Their frontmatter is not consistent from text to text. We can grab nine of our texts by using the following phrases: \"START OF THIS PROJECT GUTENBERG EBOOK.\" This won't perfectly massage out all the frontmatter, but for the sake of simplicity I will leave it as is. For the sake of practice, we'll be defining a function for doing these things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['us-ascii', ')', '***start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'jane', 'eyre***transcribed', 'from', 'the', '1897', 'service', '&', 'paton', 'edition', 'by', 'david', 'price', ',', 'email', 'ccx074', '@', 'pglaf.orgjane', 'eyrean', 'autobiographybycharlotte', 'brontëillustrated', 'by', 'f.', 'h.', 'townsendlondonservice', '&', 'paton5', 'henrietta', 'street1897the', 'illustrationsin', 'this', 'volume', 'are', 'the', 'copyright', 'ofservice', '&', 'paton', ',', 'londontow', '.', 'm.', 'thackeray', ',', 'esq.', ',', 'this', 'workis', 'respectfully', 'inscribedbythe', 'authorprefacea', 'preface', 'to', 'the', 'first', 'edition', 'of', '“jane', 'eyre”', 'being', 'unnecessary', ',', 'i', 'gave', 'none', ':', 'this', 'second', 'edition', 'demands', 'a', 'few', 'words', 'both', 'of', 'acknowledgment', 'and', 'miscellaneous', 'remark.my', 'thanks', 'are', 'due', 'in', 'three', 'quarters.to', 'the', 'public', ',', 'for', 'the', 'indulgent', 'ear', 'it', 'has', 'inclined', 'to', 'a', 'plain', 'tale', 'with', 'few', 'pretensions.to', 'the', 'press', ',', 'for', 'the', 'fair', 'field', 'its', 'honest', 'suffrage', 'has', 'opened', 'to', 'an', 'obscure', 'aspirant.to', 'my', 'publishers', ',', 'for', 'the', 'aid', 'their', 'tact', ',', 'their', 'energy', ',', 'their', 'practical', 'sense', 'and', 'frank', 'liberality', 'have', 'afforded', 'an', 'unknown', 'and', 'unrecommended', 'author.the', 'press', 'and', 'the', 'public', 'are', 'but', 'vague', 'personifications', 'for', 'me', ',', 'and', 'i', 'must', 'thank', 'them', 'in', 'vague', 'terms', ';', 'but', 'my', 'publishers', 'are', 'definite', ':', 'so', 'are', 'certain', 'generous', 'critics', 'who', 'have', 'encouraged', 'me', 'as', 'only', 'large-hearted', 'and', 'high-minded', 'men', 'know', 'how', 'to', 'encourage', 'a', 'struggling', 'stranger', ';', 'to']\n",
      "['mighty', 'victories', 'of', 'the', 'lamb', ',', 'who', 'are', 'called', ',', 'and', 'chosen', ',', 'and', 'faithful.st', '.', 'john', 'is', 'unmarried', ':', 'he', 'never', 'will', 'marry', 'now', '.', 'himself', 'has', 'hitherto', 'sufficed', 'to', 'the', 'toil', ',', 'and', 'the', 'toil', 'draws', 'near', 'its', 'close', ':', 'his', 'glorious', 'sun', 'hastens', 'to', 'its', 'setting', '.', 'the', 'last', 'letter', 'i', 'received', 'from', 'him', 'drew', 'from', 'my', 'eyes', 'human', 'tears', ',', 'and', 'yet', 'filled', 'my', 'heart', 'with', 'divine', 'joy', ':', 'he', 'anticipated', 'his', 'sure', 'reward', ',', 'his', 'incorruptible', 'crown', '.', 'i', 'know', 'that', 'a', 'stranger’s', 'hand', 'will', 'write', 'to', 'me', 'next', ',', 'to', 'say', 'that', 'the', 'good', 'and', 'faithful', 'servant', 'has', 'been', 'called', 'at', 'length', 'into', 'the', 'joy', 'of', 'his', 'lord', '.', 'and', 'why', 'weep', 'for', 'this', '?', 'no', 'fear', 'of', 'death', 'will', 'darken', 'st.', 'john’s', 'last', 'hour', ':', 'his', 'mind', 'will', 'be', 'unclouded', ',', 'his', 'heart', 'will', 'be', 'undaunted', ',', 'his', 'hope', 'will', 'be', 'sure', ',', 'his', 'faith', 'steadfast', '.', 'his', 'own', 'words', 'are', 'a', 'pledge', 'of', 'this—“my', 'master', ',', '”', 'he', 'says', ',', '“has', 'forewarned', 'me', '.', 'daily', 'he', 'announces', 'more', 'distinctly', ',', '—‘surely', 'i', 'come', 'quickly', '!', '’', 'and', 'hourly', 'i', 'more', 'eagerly', 'respond', ',', '—‘amen', ';', 'even', 'so', 'come', ',', 'lord', 'jesus', '!']\n"
     ]
    }
   ],
   "source": [
    "def normalize(tokens):\n",
    "    \"\"\"Takes a list of tokens and returns a list of tokens \n",
    "    that has been normalized by lowercasing all tokens and \n",
    "    removing Project Gutenberg frontmatter.\"\"\"\n",
    "    \n",
    "#     lowercase all words\n",
    "    normalized = [token.lower() for token in tokens]\n",
    "    \n",
    "#     very rough end of front matter.\n",
    "    end_of_front_matter = 90\n",
    "#     very rough beginning of end matter.\n",
    "    start_of_end_matter = -2973\n",
    "#     get only the text between the end matter and front matter\n",
    "    normalized = normalized[end_of_front_matter:start_of_end_matter]\n",
    "\n",
    "    return normalized\n",
    "\n",
    "print(normalize(bronte[0])[:200])\n",
    "print(normalize(bronte[0])[-200:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I've printed about the first 200 and the last 200 words of Jane Eyre to see how we're doing. Pretty rough! That's because we're mostly just guessing where the beginning and the ending of the actual text is. The problem here is that PG changes the structure of its paratext for each text, so we would need something pretty sophisticated to work through it cleanly. If you wanted a more refined approach, you could [this package](https://pypi.python.org/pypi/Gutenberg), though it has enough installation requirements that we didn't want to deal with it in this course. Essentially, it uses a lot of complicated formulae to determine how to strip off the gutenberg material. They use a syntax called **regular expressions** that is (thankfully) out of the scope of this course. For now, we'll just accept our rough cut with the understanding that we would want to clean things up more were we working on our own. \n",
    "\n",
    "Let's normalize everything with these caveats in mind. Below, we essentially say,\n",
    "\n",
    "* Go through each in the list of texts\n",
    "* For each of those texts, normalize the text in them using the function we defined above.\n",
    "* Take the results of that normalization process and make a new list out of them.\n",
    "* The result will be a list of normalized tokens stored in a variable of the same name as the original list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bring', 'forward', 'all', 'the', 'facts', '.', 'only', 'now', ',', 'at', 'the', 'end', 'of', 'nearly', 'ten', 'years', ',', 'am', 'i', 'allowed', 'to', 'supply', 'those', 'missing', 'links', 'which', 'make', 'up', 'the', 'whole']\n"
     ]
    }
   ],
   "source": [
    "doyle = [normalize(text) for text in doyle]\n",
    "bronte = [normalize(text) for text in bronte]\n",
    "\n",
    "print(doyle[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "\n",
    "The last step in this basic text analysis pipeline is to remove those words that we don't care about. The most common words in any text are articles, pronouns, and punctuation, words that might not carry a lot of information in them about the text themselves. While there are sometimes good reasons for keeping this list of **stopwords** in the text, we usually take them out to get a better read of things we actually care about in a text. NLTK actually comes with a big packet of stopwords. Let's import it and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll loop over the cleaned texts and get rid of those words that exist in the stopwords list. To do this, we'll compare both lists.\n",
    "\n",
    "Also grab the count for the text pre-stopwording to make clear how many words are lost when you do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [for token in tokens if not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Introduce basics of the NLP pipeline - normalizing, tokenizing, stopwords\n",
    "TODO: Get some basic results\n",
    "TODO: Visualize them with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
